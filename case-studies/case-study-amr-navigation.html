<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Autonomous Mobile Robot Navigation - YF Studio Case Study</title>
    <meta name="description"
        content="AI-powered obstacle avoidance and path planning for AMR systems with advanced computer vision.">

    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://yfstudiouk.github.io/case-studies/case-study-amr-navigation.html">
    <meta property="og:title" content="Autonomous Mobile Robot Navigation - YF Studio Case Study">
    <meta property="og:description"
        content="AI-powered obstacle avoidance and path planning for AMR systems with advanced computer vision.">
    <meta property="og:image" content="https://yfstudiouk.github.io/images/og-image.jpg">

    <!-- Twitter -->
    <meta property="twitter:card" content="summary_large_image">
    <meta property="twitter:url" content="https://yfstudiouk.github.io/case-studies/case-study-amr-navigation.html">
    <meta property="twitter:title" content="Autonomous Mobile Robot Navigation - YF Studio Case Study">
    <meta property="twitter:description"
        content="AI-powered obstacle avoidance and path planning for AMR systems with advanced computer vision.">
    <meta property="twitter:image" content="https://yfstudiouk.github.io/images/og-image.jpg">

    <!-- Canonical & Robots -->
    <link rel="canonical" href="https://yfstudiouk.github.io/case-studies/case-study-amr-navigation.html">
    <meta name="robots" content="index, follow">

    <link rel="stylesheet" href="../css/styles.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">

    <!-- Google Analytics -->
    <script src="../js/analytics.js"></script>


    <style>
        .case-study-hero {
            min-height: 60vh;
            display: flex;
            align-items: center;
            background:
                radial-gradient(circle at 20% 80%, rgba(0, 255, 255, 0.15) 0%, transparent 50%),
                radial-gradient(circle at 80% 20%, rgba(255, 0, 255, 0.1) 0%, transparent 50%),
                radial-gradient(circle at 40% 40%, rgba(128, 0, 255, 0.1) 0%, transparent 50%),
                radial-gradient(circle at 60% 60%, rgba(0, 0, 255, 0.08) 0%, transparent 50%),
                linear-gradient(135deg, #000000 0%, #0a0a0a 25%, #1a1a1a 50%, #0d1117 75%, #161b22 100%);
            color: #ff00ff;
            position: relative;
            overflow: hidden;
            border-bottom: 1px solid rgba(255, 0, 255, 0.2);
        }

        .case-study-content {
            padding: 4rem 0;
            background: linear-gradient(135deg, #000000 0%, #0a0a0a 25%, #1a1a1a 50%, #0d1117 75%, #161b22 100%);
            color: white;
        }

        .case-study-section {
            margin-bottom: 3rem;
        }

        .case-study-section h2 {
            background: linear-gradient(45deg, #ff00ff, #00ffff);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
            font-size: 2rem;
            margin-bottom: 1rem;
        }

        .case-study-section h3 {
            color: #00ffff;
            font-size: 1.5rem;
            margin-bottom: 1rem;
        }

        .tech-stack {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 1rem;
            margin: 2rem 0;
        }

        .tech-item {
            background: rgba(255, 0, 255, 0.1);
            border: 1px solid rgba(255, 0, 255, 0.3);
            padding: 1rem;
            border-radius: 10px;
            text-align: center;
        }

        .back-button {
            position: fixed;
            top: 20px;
            left: 20px;
            background: linear-gradient(135deg, #000000 0%, #1a1a1a 100%);
            color: #ff00ff;
            border: 2px solid rgba(255, 0, 255, 0.4);
            padding: 0.8rem 1.5rem;
            border-radius: 25px;
            text-decoration: none;
            font-weight: 600;
            box-shadow: 0 5px 15px rgba(255, 0, 255, 0.3);
            transition: all 0.3s ease;
            z-index: 1000;
        }

        .back-button:hover {
            background: linear-gradient(135deg, #ff00ff, #00ffff);
            color: #000000;
            transform: translateY(-2px);
            box-shadow: 0 8px 25px rgba(255, 0, 255, 0.5);
        }

        .case-study-section ul {
            padding-left: 2rem;
        }

        .case-study-section li {
            margin-bottom: 0.5rem;
        }

        .case-study-section h3 {
            margin-top: 2rem;
            margin-bottom: 1rem;
        }

        .case-study-section h3:first-child {
            margin-top: 0;
        }
    </style>
</head>

<body>
    <a href="../index.html" class="back-button">
        <i class="fas fa-arrow-left"></i> Back to Portfolio
    </a>

    <!-- Case Study Hero -->
    <section class="case-study-hero">
        <div class="container">
            <div class="hero-content">
                <h1 class="hero-title">
                    Autonomous Mobile Robot Navigation
                    <span class="gradient-text">AI-Powered AMR System</span>
                </h1>
                <p class="hero-description">
                    Advanced obstacle avoidance and path planning system for autonomous mobile robots
                    with real-time computer vision and LiDAR fusion, achieving 99.7% obstacle detection
                    across three distribution centre environments.
                </p>
            </div>
        </div>
    </section>

    <!-- Case Study Content -->
    <section class="case-study-content">
        <div class="container">
            <!-- Project Context -->
            <div class="case-study-section">
                <h2>Project Context</h2>
                <p>A regional logistics operator with 3 distribution centres across Yorkshire engaged YF Studio for an
                    8-month project from June 2023 to January 2024. The client handles parcel sorting and pallet
                    movement for regional e-commerce fulfilment, operating 18-hour shifts across their facilities.
                    Prior to this engagement, all internal material transport was handled by manned forklifts and
                    manual pallet jacks, creating bottlenecks during peak periods and contributing to a high rate of
                    minor workplace incidents (averaging 3.2 per quarter across all sites).</p>
                <div class="tech-stack">
                    <div class="tech-item">
                        <h4>Timeline</h4>
                        <p>8 months (Jun 2023 &ndash; Jan 2024)</p>
                    </div>
                    <div class="tech-item">
                        <h4>Team</h4>
                        <p>1 robotics engineer, 1 CV specialist, 1 embedded systems developer</p>
                    </div>
                    <div class="tech-item">
                        <h4>Industry</h4>
                        <p>Regional logistics &amp; e-commerce fulfilment</p>
                    </div>
                </div>
            </div>

            <!-- Project Overview -->
            <div class="case-study-section">
                <h2>Project Overview</h2>
                <p>Developed a comprehensive autonomous navigation system for industrial AMRs operating in dynamic
                    warehouse environments. The system combines computer vision, sensor fusion, and machine learning to
                    enable
                    safe autonomous operation in shared human-robot workspaces.</p>

                <div class="tech-stack">
                    <div class="tech-item">
                        <h4>Hardware</h4>
                        <p>Custom AMR Platform, NVIDIA Jetson AGX Orin, 2D/3D LiDAR, Wheel Encoders</p>
                    </div>
                    <div class="tech-item">
                        <h4>AI Framework</h4>
                        <p>ROS2, PCL, OpenCV, TensorFlow Lite</p>
                    </div>
                    <div class="tech-item">
                        <h4>Computer Vision</h4>
                        <p>YOLOv7, SLAM, Depth Estimation, Object Tracking</p>
                    </div>
                    <div class="tech-item">
                        <h4>Navigation</h4>
                        <p>A* Pathfinding, RRT*, Dynamic Window Approach</p>
                    </div>
                </div>
            </div>

            <!-- Challenge -->
            <div class="case-study-section">
                <h2>The Challenge</h2>
                <p>The client had previously trialled a commercially available AMR platform from a European vendor in
                    2022 at their largest distribution centre. The off-the-shelf system relied on fixed magnetic tape
                    guidance with limited obstacle response &mdash; when encountering an unexpected object, the robot
                    would simply halt and wait for manual clearance. During peak season, this resulted in an average of
                    14 stoppages per shift, effectively negating the throughput benefits of automation. The system was
                    also unable to handle the dynamic racking reconfigurations the client performs monthly, requiring
                    expensive re-mapping by the vendor each time. After 5 months, the pilot was discontinued.</p>
                <p>The client approached YF Studio to develop a truly autonomous solution that could navigate
                    dynamically without fixed infrastructure. Key challenges included:</p>
                <ul>
                    <li>Navigation in cluttered warehouse aisles with variable racking configurations</li>
                    <li>Real-time obstacle detection for forklifts, pedestrians, dropped pallets, and shrink-wrap debris</li>
                    <li>Dynamic path planning that adapts to weekly layout changes without manual re-mapping</li>
                    <li>Precise localisation (sub-5cm) in GPS-denied steel-framed buildings</li>
                    <li>Integration with the client's existing Warehouse Management System (WMS) via REST API</li>
                    <li>Full compliance with ISO 3691-4 safety standards for industrial AMRs in shared workspaces</li>
                </ul>
            </div>

            <!-- Solution -->
            <div class="case-study-section">
                <h2>Our Solution</h2>
                <h3>1. Multi-Sensor Fusion System</h3>
                <p>Integrated stereo cameras, a 2D LiDAR (SICK TiM781), an IMU, and ultrasonic proximity sensors into
                    a unified perception pipeline. We chose the SICK TiM781 over the Hokuyo UST-10LX due to its
                    superior angular resolution (0.33&deg;) at the required range, and its IP65 rating suited to the
                    dusty warehouse environment. Camera-only approaches were evaluated but rejected because of the
                    highly variable lighting across the three facilities &mdash; some areas are well-lit while loading
                    dock zones are intermittently dark. The fusion architecture uses an Extended Kalman Filter to
                    combine sensor modalities, providing robust localisation even when individual sensors degrade
                    (e.g., LiDAR reflections on wet floors, camera exposure shifts near dock doors).</p>

                <h3>2. Real-time Obstacle Detection</h3>
                <p>Developed a lightweight YOLOv7-tiny model quantised to INT8 via TensorRT for deployment on the
                    NVIDIA Jetson AGX Orin. YOLOv7-tiny was selected over YOLOv8-nano after benchmarking showed it
                    achieved comparable mAP (91.3% vs. 92.1%) with 20% lower inference latency on the target hardware,
                    which was critical for meeting the 15ms detection budget. The model classifies 8 obstacle
                    categories: pedestrians, forklifts, pallet jacks, pallets (loaded/empty), racking, debris, and
                    unknown-dynamic. A DeepSORT tracker maintains object identity across frames for velocity estimation,
                    enabling predictive avoidance of moving obstacles.</p>

                <h3>3. Dynamic Path Planning</h3>
                <p>Implemented a two-layer planning architecture: A* on a regularly updated occupancy grid for global
                    route planning, and Dynamic Window Approach (DWA) for real-time local obstacle avoidance. We
                    evaluated RRT* as an alternative global planner but found A* produced more predictable, human-readable
                    paths in the structured aisle environment &mdash; an important factor for safety certification and
                    operator trust. The global map is incrementally updated from SLAM data, meaning the system
                    automatically adapts to racking reconfigurations without requiring manual re-mapping.</p>

                <h3>4. SLAM Integration</h3>
                <p>Integrated Cartographer SLAM (Google's open-source library) for simultaneous localisation and mapping
                    in the GPS-denied warehouse environment. Cartographer was chosen over GMapping because of its
                    superior loop-closure performance in large facilities (the client's largest site is 12,000 m&sup2;).
                    The system builds and maintains 2D occupancy grid maps while tracking the robot's position with
                    2cm accuracy using LiDAR scan-matching and wheel odometry. Map updates are shared across the fleet
                    via a central ROS2 map server, so when one robot detects a layout change, all robots receive the
                    update within 5 seconds.</p>
            </div>

            <!-- Technical Implementation -->
            <div class="case-study-section">
                <h2>Technical Implementation</h2>
                <h3>Perception Pipeline</h3>
                <p>The perception system processes multiple sensor inputs:</p>
                <ul>
                    <li><strong>Visual Processing:</strong> RGB and depth image analysis for obstacle detection</li>
                    <li><strong>LiDAR Processing:</strong> Point cloud analysis for 3D mapping and obstacle detection
                    </li>
                    <li><strong>Sensor Fusion:</strong> Kalman filtering for robust state estimation</li>
                    <li><strong>Object Tracking:</strong> Multi-object tracking for dynamic obstacle avoidance</li>
                </ul>

                <h3>Navigation Architecture</h3>
                <p>The navigation system consists of:</p>
                <ul>
                    <li>Global path planner (A* on occupancy grid) for facility-wide navigation</li>
                    <li>Local path planner (DWA) for dynamic obstacle avoidance with 10Hz replanning</li>
                    <li>Kinematic controller for smooth trapezoidal motion profiles with jerk limiting</li>
                    <li>Safety-rated stop system with dual-channel redundancy (SIL 2 rated)</li>
                </ul>

                <h3>Limitations &amp; Edge Cases</h3>
                <p>Thorough field testing across all three distribution centres identified several operational
                    limitations that are documented in the operator handbook:</p>
                <ul>
                    <li><strong>Wet-floor conditions:</strong> Performance in wet-floor conditions drops to 94%
                        obstacle detection accuracy (vs. 99.7% on dry surfaces) due to LiDAR beam reflections off
                        pooled water creating phantom obstacles. The system compensates by switching to
                        camera-primary mode when floor reflectance anomalies are detected, though this increases
                        detection latency to approximately 25ms.</li>
                    <li><strong>Smoke and steam:</strong> Steam venting from nearby loading dock heating systems and
                        exhaust from delivery vehicles occasionally triggers false positive obstacle detections. This
                        was addressed through temporal filtering &mdash; transient detections that do not persist across
                        3 consecutive frames (150ms) are suppressed. This reduces false emergency stops by 87% with
                        negligible impact on genuine obstacle response time.</li>
                    <li><strong>Narrow aisles:</strong> Aisles below 1.2m width require the system to enter reduced
                        speed mode (0.3 m/s vs. the standard 1.5 m/s) due to limited lateral clearance for obstacle
                        avoidance manoeuvres. This affects approximately 15% of routes in the client's oldest facility
                        where racking was installed before AMR deployment was planned.</li>
                    <li><strong>Battery constraints:</strong> Battery-constrained operation limits autonomous runtime
                        to 6.5 hours before requiring dock return for charging, compared to the client's 18-hour shift
                        pattern. This is mitigated by deploying robots in staggered charging rotation, but means a
                        minimum fleet size of 3 robots is needed per facility for continuous coverage.</li>
                    <li><strong>Highly reflective racking:</strong> New galvanised steel racking installed in one
                        facility wing produced LiDAR multi-path reflections that degraded localisation accuracy to
                        approximately 8cm (vs. the standard 2cm). Applying anti-reflective tape to racking uprights at
                        LiDAR height resolved the issue.</li>
                </ul>
            </div>

            <!-- Results -->
            <div class="case-study-section">
                <h2>Results &amp; Impact</h2>
                <div class="tech-stack">
                    <div class="tech-item">
                        <h4>99.7%</h4>
                        <p>Obstacle Detection (dry surfaces)</p>
                        <p style="font-size: 0.8rem; color: #aaa;">Previous system: halt-and-wait only</p>
                    </div>
                    <div class="tech-item">
                        <h4>15ms</h4>
                        <p>Obstacle Detection Latency</p>
                        <p style="font-size: 0.8rem; color: #aaa;">Previous system: N/A (no detection)</p>
                    </div>
                    <div class="tech-item">
                        <h4>2cm</h4>
                        <p>Localisation Accuracy</p>
                        <p style="font-size: 0.8rem; color: #aaa;">Previous system: fixed-path only</p>
                    </div>
                    <div class="tech-item">
                        <h4>6.5 hours</h4>
                        <p>Continuous Operation Time</p>
                        <p style="font-size: 0.8rem; color: #aaa;">Per charge cycle</p>
                    </div>
                </div>

                <h3>Performance Metrics</h3>
                <ul>
                    <li>Successfully completed 5,000+ km of autonomous travel over 14 months of continuous operation
                        across all three distribution centres</li>
                    <li>Reduced material handling labour costs by 35% through partial automation of internal pallet
                        transport (remaining 65% of cost is manned forklift operations for elevated storage)</li>
                    <li>Achieved 99.7% obstacle detection rate on dry surfaces in mixed human-robot traffic
                        (94% in wet-floor conditions, see Limitations)</li>
                    <li>Maintained 2cm docking accuracy for pallet pick-up and drop-off points</li>
                    <li>Processed obstacle detection in under 15ms end-to-end on Jetson AGX Orin</li>
                    <li>Reduced warehouse workplace incidents from 3.2 to 0.8 per quarter (75% reduction) by
                        removing manned pallet jack trips from high-traffic zones</li>
                </ul>
            </div>

            <!-- Safety Features -->
            <div class="case-study-section">
                <h2>Safety Features</h2>
                <p>The system includes comprehensive safety mechanisms:</p>
                <ul>
                    <li><strong>Emergency Stop:</strong> Hardware-level e-stop for immediate halting</li>
                    <li><strong>Safety Zones:</strong> Dynamic speed reduction in high-traffic areas</li>
                    <li><strong>Battery Management:</strong> Autonomous charging when battery is low</li>
                    <li><strong>Load Stability:</strong> Active monitoring of payload stability</li>
                    <li><strong>Manual Override:</strong> Joystick control for manual maneuvering</li>
                </ul>
            </div>

            <!-- Applications -->
            <div class="case-study-section">
                <h2>Applications</h2>
                <p>The autonomous navigation system enables various applications:</p>
                <ul>
                    <li><strong>Intralogistics:</strong> Automated material transport in factories</li>
                    <li><strong>E-commerce Fulfillment:</strong> Goods-to-person picking systems</li>
                    <li><strong>Hospital Logistics:</strong> Autonomous delivery of medicine and linens</li>
                    <li><strong>Retail Inventory:</strong> Shelf scanning and stock monitoring</li>
                    <li><strong>Disinfection:</strong> Autonomous UV-C disinfection robots</li>
                </ul>
            </div>

            <!-- Ongoing & Next Steps -->
            <div class="case-study-section">
                <h2>Ongoing &amp; Next Steps</h2>
                <p>The system has been in production operation since January 2024. YF Studio provides ongoing support
                    and is working with the client on the following enhancements:</p>
                <ul>
                    <li><strong>Fleet coordination (in progress):</strong> Expanding from single-robot operation to a
                        coordinated fleet of 4 robots per facility, with centralised task allocation and traffic
                        management to prevent aisle congestion. Initial multi-robot testing began in Q2 2024.</li>
                    <li><strong>Opportunity charging:</strong> Implementing short-burst charging at intermediate
                        docking points to extend effective operational runtime beyond the current 6.5-hour limit,
                        targeting 95% uptime during 18-hour shifts.</li>
                    <li><strong>Semantic mapping:</strong> Augmenting the occupancy grid with semantic labels
                        (loading zone, pedestrian crossing, forklift-only aisle) to enable context-aware speed and
                        behaviour policies rather than relying solely on geometric obstacle avoidance.</li>
                    <li><strong>Wet-floor detection model:</strong> Training a dedicated floor-condition classifier
                        to proactively switch sensor modes before LiDAR degradation occurs, rather than reacting to
                        anomalies after the fact.</li>
                    <li><strong>Third-party WMS integration expansion:</strong> The client is migrating from their
                        legacy WMS to a cloud-based system; YF Studio is developing an updated API adapter to maintain
                        seamless task dispatch.</li>
                </ul>
            </div>
        </div>
    </section>


</body>

</html>