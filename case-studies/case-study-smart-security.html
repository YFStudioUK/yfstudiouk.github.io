<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Smart Security Camera - YF Studio Case Study</title>
    <meta name="description"
        content="Edge AI surveillance system with privacy-first facial recognition and person detection.">

    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://yfstudiouk.github.io/case-studies/case-study-smart-security.html">
    <meta property="og:title" content="Smart Security Camera - YF Studio Case Study">
    <meta property="og:description"
        content="Edge AI surveillance system with privacy-first facial recognition and person detection.">
    <meta property="og:image" content="https://yfstudiouk.github.io/images/og-image.jpg">

    <!-- Twitter -->
    <meta property="twitter:card" content="summary_large_image">
    <meta property="twitter:url" content="https://yfstudiouk.github.io/case-studies/case-study-smart-security.html">
    <meta property="twitter:title" content="Smart Security Camera - YF Studio Case Study">
    <meta property="twitter:description"
        content="Edge AI surveillance system with privacy-first facial recognition and person detection.">
    <meta property="twitter:image" content="https://yfstudiouk.github.io/images/og-image.jpg">

    <!-- Canonical & Robots -->
    <link rel="canonical" href="https://yfstudiouk.github.io/case-studies/case-study-smart-security.html">
    <meta name="robots" content="index, follow">

    <link rel="stylesheet" href="../css/styles.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">

    <!-- Google Analytics -->
    <script src="../js/analytics.js"></script>


    <style>
        .case-study-hero {
            min-height: 60vh;
            display: flex;
            align-items: center;
            background:
                radial-gradient(circle at 20% 80%, rgba(0, 255, 255, 0.15) 0%, transparent 50%),
                radial-gradient(circle at 80% 20%, rgba(255, 0, 255, 0.1) 0%, transparent 50%),
                radial-gradient(circle at 40% 40%, rgba(128, 0, 255, 0.1) 0%, transparent 50%),
                radial-gradient(circle at 60% 60%, rgba(0, 0, 255, 0.08) 0%, transparent 50%),
                linear-gradient(135deg, #000000 0%, #0a0a0a 25%, #1a1a1a 50%, #0d1117 75%, #161b22 100%);
            color: #ff00ff;
            position: relative;
            overflow: hidden;
            border-bottom: 1px solid rgba(255, 0, 255, 0.2);
        }

        .case-study-content {
            padding: 4rem 0;
            background: linear-gradient(135deg, #000000 0%, #0a0a0a 25%, #1a1a1a 50%, #0d1117 75%, #161b22 100%);
            color: white;
        }

        .case-study-section {
            margin-bottom: 3rem;
        }

        .case-study-section h2 {
            background: linear-gradient(45deg, #ff00ff, #00ffff);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
            font-size: 2rem;
            margin-bottom: 1rem;
        }

        .case-study-section h3 {
            color: #00ffff;
            font-size: 1.5rem;
            margin-bottom: 1rem;
        }

        .tech-stack {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 1rem;
            margin: 2rem 0;
        }

        .tech-item {
            background: rgba(255, 0, 255, 0.1);
            border: 1px solid rgba(255, 0, 255, 0.3);
            padding: 1rem;
            border-radius: 10px;
            text-align: center;
        }

        .back-button {
            position: fixed;
            top: 20px;
            left: 20px;
            background: linear-gradient(135deg, #000000 0%, #1a1a1a 100%);
            color: #ff00ff;
            border: 2px solid rgba(255, 0, 255, 0.4);
            padding: 0.8rem 1.5rem;
            border-radius: 25px;
            text-decoration: none;
            font-weight: 600;
            box-shadow: 0 5px 15px rgba(255, 0, 255, 0.3);
            transition: all 0.3s ease;
            z-index: 1000;
        }

        .back-button:hover {
            background: linear-gradient(135deg, #ff00ff, #00ffff);
            color: #000000;
            transform: translateY(-2px);
            box-shadow: 0 8px 25px rgba(255, 0, 255, 0.5);
        }

        .case-study-section ul {
            padding-left: 2rem;
        }

        .case-study-section li {
            margin-bottom: 0.5rem;
        }

        .case-study-section h3 {
            margin-top: 2rem;
            margin-bottom: 1rem;
        }

        .case-study-section h3:first-child {
            margin-top: 0;
        }
    </style>
</head>

<body>
    <a href="../index.html" class="back-button">
        <i class="fas fa-arrow-left"></i> Back to Portfolio
    </a>

    <!-- Case Study Hero -->
    <section class="case-study-hero">
        <div class="container">
            <div class="hero-content">
                <h1 class="hero-title">
                    Smart Security Camera System
                    <span class="gradient-text">Edge AI Solution</span>
                </h1>
                <p class="hero-description">
                    Real-time person detection and facial recognition system deployed on edge devices
                    for intelligent security monitoring with 99.2% daytime accuracy and sub-50ms inference on Jetson Nano.
                </p>
            </div>
        </div>
    </section>

    <!-- Case Study Content -->
    <section class="case-study-content">
        <div class="container">
            <!-- Project Context -->
            <div class="case-study-section">
                <h2>Project Context</h2>
                <div class="tech-stack">
                    <div class="tech-item">
                        <h4>Timeline</h4>
                        <p>12 months, Q1 2023 &ndash; Q1 2024</p>
                    </div>
                    <div class="tech-item">
                        <h4>Team</h4>
                        <p>2 ML Engineers, 1 Edge Deployment Specialist, 1 Security Domain Consultant</p>
                    </div>
                    <div class="tech-item">
                        <h4>Client</h4>
                        <p>UK-based commercial security integrator serving retail and hospitality</p>
                    </div>
                    <div class="tech-item">
                        <h4>Scope</h4>
                        <p>Pilot: 15 sites &rarr; Full rollout: 500+ locations</p>
                    </div>
                </div>
                <p>The client provides integrated security services to national retail chains and hospitality groups.
                    Their existing CCTV infrastructure relied on motion-triggered recording with high false-alarm rates,
                    leading to alert fatigue among monitoring staff and missed genuine incidents. They needed an AI layer
                    that could run on their existing camera hardware with minimal per-site cost.</p>
            </div>

            <!-- Project Overview -->
            <div class="case-study-section">
                <h2>Project Overview</h2>
                <p>Developed an edge AI security camera system capable of real-time person detection and optional facial
                    recognition for authorised personnel. The system processes video streams locally on embedded
                    hardware, eliminating the need for cloud connectivity while maintaining high accuracy and low
                    latency. The project was scoped specifically for person detection and face matching &mdash; not for
                    crowd counting or general behaviour analytics.</p>

                <div class="tech-stack">
                    <div class="tech-item">
                        <h4>Hardware</h4>
                        <p>NVIDIA Jetson Nano, Raspberry Pi 4, Custom ARM SoC</p>
                    </div>
                    <div class="tech-item">
                        <h4>AI Framework</h4>
                        <p>TensorFlow Lite, OpenVINO, ONNX Runtime</p>
                    </div>
                    <div class="tech-item">
                        <h4>Computer Vision</h4>
                        <p>OpenCV, YOLOv7, FaceNet, DeepSORT</p>
                    </div>
                    <div class="tech-item">
                        <h4>Deployment</h4>
                        <p>Docker, Remote OTA Updates, Edge Computing</p>
                    </div>
                </div>
            </div>

            <!-- Challenge -->
            <div class="case-study-section">
                <h2>The Challenge</h2>
                <p>A UK-based commercial security integrator serving retail and hospitality needed to upgrade their
                    surveillance infrastructure with AI-powered person detection and face matching while maintaining
                    strict GDPR compliance and reducing bandwidth costs. Key requirements included:</p>
                <ul>
                    <li>Real-time person detection with 95%+ accuracy across day and night conditions</li>
                    <li>Optional facial recognition for authorised personnel at access-controlled areas</li>
                    <li>On-device processing to ensure no biometric data leaves the edge unit</li>
                    <li>Integration with existing VMS (video management systems) and alarm panels</li>
                    <li>Cost-effective deployment across 500+ retail and hospitality locations</li>
                    <li>Remote management and OTA firmware updates for all deployed units</li>
                </ul>

                <h3>What Had Been Tried Before</h3>
                <p>The client had previously trialled a cloud-based video analytics service from a major CCTV vendor.
                    While detection accuracy was adequate, the solution required continuous video upload to remote
                    servers &mdash; creating unacceptable bandwidth costs (averaging 8 Mbps per camera) and raising GDPR
                    concerns around off-site biometric processing. Latency between detection and alert was over 3
                    seconds due to round-trip network delays. The per-camera licensing model also made national rollout
                    economically unviable. The client needed an on-premise, per-site-licensed alternative that could run
                    on low-cost hardware.</p>
            </div>

            <!-- Solution -->
            <div class="case-study-section">
                <h2>Our Solution</h2>
                <h3>1. Custom AI Model Development</h3>
                <p>Developed a lightweight YOLOv7-tiny-based person detection model optimised for edge deployment. The
                    model was trained on a curated dataset combining COCO person annotations with 12,000 frames captured
                    from the client's own camera feeds (covering indoor retail, outdoor car parks, and loading bays).
                    Post-training quantization to INT8 precision reduced model size by 75% while maintaining 99.2%
                    person detection accuracy in daytime conditions.</p>
                <p><strong>Why YOLOv7-tiny:</strong> We evaluated YOLOv5s, YOLOv7-tiny, and EfficientDet-Lite. YOLOv7-tiny
                    offered the best accuracy-to-latency ratio on Jetson Nano (45ms inference vs. 62ms for YOLOv5s and
                    85ms for EfficientDet-Lite at comparable mAP). Its architecture also quantises more cleanly to INT8
                    with minimal accuracy loss compared to EfficientDet's depthwise separable convolutions.</p>

                <h3>2. Facial Recognition Pipeline</h3>
                <p>Implemented a FaceNet-based recognition system for optional face matching at access-controlled
                    entry points. The system enrols authorised personnel via a secure admin interface and stores only
                    128-dimensional encrypted feature vectors &mdash; never raw images. It achieves 97.5% true-positive
                    matching in controlled indoor lighting and 93.1% in mixed outdoor conditions.</p>
                <p><strong>Why FaceNet over ArcFace:</strong> Although ArcFace achieves marginally higher accuracy on
                    benchmark datasets, FaceNet's embedding model is 40% smaller (23MB vs. 39MB) and runs within the
                    Jetson Nano's memory budget alongside the detection model. For the client's use case (matching
                    against a database of fewer than 200 authorised personnel per site), FaceNet's discriminative power
                    is more than sufficient.</p>

                <h3>3. Edge Optimisation</h3>
                <p>Optimised the entire pipeline for ARM-based processors using TensorFlow Lite (Raspberry Pi 4) and
                    TensorRT (Jetson Nano). Achieved inference times of 45ms per frame on NVIDIA Jetson Nano and 78ms
                    on Raspberry Pi 4.</p>
                <p><strong>Why TensorRT on Jetson, TFLite on Pi:</strong> TensorRT leverages the Jetson Nano's 128-core
                    Maxwell GPU for INT8 acceleration &mdash; something TFLite cannot exploit. On Raspberry Pi 4 (CPU-only),
                    TFLite's XNNPACK delegate outperforms ONNX Runtime by approximately 15% on ARM Cortex-A72. This
                    dual-runtime approach maximises performance on both hardware tiers without maintaining two separate
                    model architectures.</p>

                <h3>4. Phased Deployment &amp; Remote Management</h3>
                <p>Phase 1 pilot covered 15 flagship retail sites over 3 months, validating detection accuracy and
                    false-alarm rates in real-world conditions. Phase 2 expanded to 120 sites in the South East,
                    introducing the remote OTA update pipeline. Phase 3 completed the national rollout to 500+
                    locations with a fully automated remote deployment pipeline &mdash; each new site requires only a
                    2-hour on-site setup (mounting the edge unit, connecting to the existing camera feed, and running
                    the automated calibration script).</p>

                <h3>5. GDPR Compliance &amp; Privacy Architecture</h3>
                <p>All facial recognition processing occurs on-device &mdash; no biometric data leaves the edge unit.
                    The system stores only encrypted feature vectors (not images) with automatic 30-day expiry. A
                    privacy impact assessment was completed with the client's Data Protection Officer, and signage
                    complying with ICO guidance is provided for all installation sites. The face matching feature is
                    entirely optional and disabled by default &mdash; sites that do not require it run person detection
                    only, with no biometric processing whatsoever.</p>
            </div>

            <!-- Technical Implementation -->
            <div class="case-study-section">
                <h2>Technical Implementation</h2>
                <h3>Architecture</h3>
                <p>The system consists of three main components:</p>
                <ul>
                    <li><strong>Capture Module:</strong> Handles video input from IP cameras (RTSP) and USB devices,
                        with automatic resolution and frame-rate negotiation</li>
                    <li><strong>AI Processing Engine:</strong> Runs detection (YOLOv7-tiny) and optional recognition
                        (FaceNet) models with DeepSORT multi-object tracking</li>
                    <li><strong>Alert System:</strong> Manages notifications via the client's existing VMS and alarm
                        panels, with configurable alert thresholds per zone</li>
                </ul>

                <h3>Model Optimisation</h3>
                <p>Applied optimisation techniques to achieve real-time performance on low-cost hardware:</p>
                <ul>
                    <li>Structured pruning to remove 30% of convolutional filters with less than 1% accuracy impact</li>
                    <li>Post-training quantization from FP32 to INT8 (TensorRT) and dynamic-range quantization (TFLite)</li>
                    <li>Knowledge distillation from a full YOLOv7 teacher model to the YOLOv7-tiny student</li>
                    <li>TensorRT layer fusion and kernel auto-tuning for Jetson Nano's Maxwell GPU</li>
                </ul>

                <h3>Limitations &amp; Edge Cases</h3>
                <p>The system has known performance boundaries that were documented and communicated to the client
                    during the pilot phase:</p>
                <ul>
                    <li><strong>Nighttime IR mode:</strong> Person detection accuracy drops to 96.8% (vs. 99.2% in
                        daylight) due to reduced contrast in infrared imagery. This remains well above the client's
                        95% threshold.</li>
                    <li><strong>Heavy rain:</strong> Effective detection range reduces from 30m to approximately 18m
                        in heavy rainfall, as water droplets on the lens housing scatter IR illumination.</li>
                    <li><strong>Crowded scenes:</strong> When more than 15 people are simultaneously visible in a
                        single frame, processing latency increases to approximately 45ms on Jetson Nano (from the
                        typical 30ms) but detection accuracy is maintained.</li>
                    <li><strong>Maximum tracked individuals:</strong> The DeepSORT tracker supports a maximum of 25
                        simultaneously tracked individuals per camera. Beyond this, the oldest tracks are dropped.</li>
                    <li><strong>Scope limitation:</strong> The system is not designed for crowd counting or behaviour
                        analytics &mdash; it focuses exclusively on person detection and optional face matching.</li>
                </ul>
            </div>

            <!-- Results -->
            <div class="case-study-section">
                <h2>Results &amp; Impact</h2>
                <div class="tech-stack">
                    <div class="tech-item">
                        <h4>99.2%</h4>
                        <p>Person Detection Accuracy (daytime)</p>
                    </div>
                    <div class="tech-item">
                        <h4>45ms &rarr; 30ms</h4>
                        <p>Inference Time (Jetson Nano, after TensorRT optimisation)</p>
                    </div>
                    <div class="tech-item">
                        <h4>75%</h4>
                        <p>Bandwidth Reduction vs. Cloud Upload</p>
                    </div>
                    <div class="tech-item">
                        <h4>500+</h4>
                        <p>Deployed Locations (national rollout)</p>
                    </div>
                </div>

                <h3>Before &amp; After Comparison</h3>
                <div class="tech-stack">
                    <div class="tech-item">
                        <h4>Before</h4>
                        <p>Motion-triggered recording: ~60% true-positive rate, 40+ false alarms/site/day</p>
                    </div>
                    <div class="tech-item">
                        <h4>After</h4>
                        <p>AI person detection: 99.2% true-positive rate, ~6 false alarms/site/day</p>
                    </div>
                    <div class="tech-item">
                        <h4>Before</h4>
                        <p>Cloud video analytics: 3+ second alert latency, 8 Mbps/camera bandwidth</p>
                    </div>
                    <div class="tech-item">
                        <h4>After</h4>
                        <p>Edge processing: &lt;100ms alert latency, metadata-only upload (&lt;50 Kbps)</p>
                    </div>
                </div>

                <h3>Business Impact</h3>
                <ul>
                    <li>Reduced false alarms by 85% (from ~40/site/day to ~6/site/day) through AI-based person detection replacing simple motion triggers</li>
                    <li>Decreased bandwidth costs by 75% by processing video on-device and transmitting only metadata and alert thumbnails</li>
                    <li>Improved alert-to-response time from over 3 seconds (cloud) to under 100ms (edge)</li>
                    <li>Achieved full GDPR compliance with on-device biometric processing and ICO-compliant signage</li>
                    <li>National rollout completed across 500+ sites with 2-hour average per-site installation time</li>
                </ul>
            </div>

            <!-- Ongoing & Next Steps -->
            <div class="case-study-section">
                <h2>Ongoing &amp; Next Steps</h2>
                <p>The system is in active operation across all 500+ sites with ongoing support and iterative
                    improvements:</p>
                <ul>
                    <li><strong>In progress:</strong> Multi-camera person re-identification across adjacent cameras
                        within a single site (e.g., tracking a person from car park to entrance)</li>
                    <li><strong>Scheduled Q2 2025:</strong> Migration from YOLOv7-tiny to YOLOv8n for improved
                        accuracy on the same hardware budget</li>
                    <li><strong>Under evaluation:</strong> Integration with the client's access control system to
                        correlate face match events with door entry logs</li>
                    <li><strong>Ongoing:</strong> Monthly model performance reviews using a sample of anonymised
                        detection logs to identify drift and edge cases</li>
                    <li><strong>Exploring:</strong> Thermal camera integration for improved nighttime detection
                        performance in outdoor environments</li>
                </ul>
            </div>
        </div>
    </section>
</body>

</html>