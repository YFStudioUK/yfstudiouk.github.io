<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Robotic Vision System - YF Studio Case Study</title>
    <meta name="description"
        content="3D vision system for robotic pick-and-place operations with sub-millimeter precision.">

    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://yfstudiouk.github.io/case-studies/case-study-robotic-vision.html">
    <meta property="og:title" content="Robotic Vision System - YF Studio Case Study">
    <meta property="og:description"
        content="3D vision system for robotic pick-and-place operations with sub-millimeter precision.">
    <meta property="og:image" content="https://yfstudiouk.github.io/images/og-image.jpg">

    <!-- Twitter -->
    <meta property="twitter:card" content="summary_large_image">
    <meta property="twitter:url" content="https://yfstudiouk.github.io/case-studies/case-study-robotic-vision.html">
    <meta property="twitter:title" content="Robotic Vision System - YF Studio Case Study">
    <meta property="twitter:description"
        content="3D vision system for robotic pick-and-place operations with sub-millimeter precision.">
    <meta property="twitter:image" content="https://yfstudiouk.github.io/images/og-image.jpg">

    <!-- Canonical & Robots -->
    <link rel="canonical" href="https://yfstudiouk.github.io/case-studies/case-study-robotic-vision.html">
    <meta name="robots" content="index, follow">

    <link rel="stylesheet" href="../css/styles.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">

    <!-- Google Analytics -->
    <script src="../js/analytics.js"></script>


    <style>
        .case-study-hero {
            min-height: 60vh;
            display: flex;
            align-items: center;
            background:
                radial-gradient(circle at 20% 80%, rgba(0, 255, 255, 0.15) 0%, transparent 50%),
                radial-gradient(circle at 80% 20%, rgba(255, 0, 255, 0.1) 0%, transparent 50%),
                radial-gradient(circle at 40% 40%, rgba(128, 0, 255, 0.1) 0%, transparent 50%),
                radial-gradient(circle at 60% 60%, rgba(0, 0, 255, 0.08) 0%, transparent 50%),
                linear-gradient(135deg, #000000 0%, #0a0a0a 25%, #1a1a1a 50%, #0d1117 75%, #161b22 100%);
            color: #ff00ff;
            position: relative;
            overflow: hidden;
            border-bottom: 1px solid rgba(255, 0, 255, 0.2);
        }

        .case-study-content {
            padding: 4rem 0;
            background: linear-gradient(135deg, #000000 0%, #0a0a0a 25%, #1a1a1a 50%, #0d1117 75%, #161b22 100%);
            color: white;
        }

        .case-study-section {
            margin-bottom: 3rem;
        }

        .case-study-section h2 {
            background: linear-gradient(45deg, #ff00ff, #00ffff);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
            font-size: 2rem;
            margin-bottom: 1rem;
        }

        .case-study-section h3 {
            color: #00ffff;
            font-size: 1.5rem;
            margin-bottom: 1rem;
        }

        .tech-stack {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 1rem;
            margin: 2rem 0;
        }

        .tech-item {
            background: rgba(255, 0, 255, 0.1);
            border: 1px solid rgba(255, 0, 255, 0.3);
            padding: 1rem;
            border-radius: 10px;
            text-align: center;
        }

        .back-button {
            position: fixed;
            top: 20px;
            left: 20px;
            background: linear-gradient(135deg, #000000 0%, #1a1a1a 100%);
            color: #ff00ff;
            border: 2px solid rgba(255, 0, 255, 0.4);
            padding: 0.8rem 1.5rem;
            border-radius: 25px;
            text-decoration: none;
            font-weight: 600;
            box-shadow: 0 5px 15px rgba(255, 0, 255, 0.3);
            transition: all 0.3s ease;
            z-index: 1000;
        }

        .back-button:hover {
            background: linear-gradient(135deg, #ff00ff, #00ffff);
            color: #000000;
            transform: translateY(-2px);
            box-shadow: 0 8px 25px rgba(255, 0, 255, 0.5);
        }

        .case-study-section ul {
            padding-left: 2rem;
        }

        .case-study-section li {
            margin-bottom: 0.5rem;
        }

        .case-study-section h3 {
            margin-top: 2rem;
            margin-bottom: 1rem;
        }

        .case-study-section h3:first-child {
            margin-top: 0;
        }
    </style>
</head>

<body>
    <a href="../index.html" class="back-button">
        <i class="fas fa-arrow-left"></i> Back to Portfolio
    </a>

    <!-- Case Study Hero -->
    <section class="case-study-hero">
        <div class="container">
            <div class="hero-content">
                <h1 class="hero-title">
                    Robotic Vision System
                    <span class="gradient-text">Intelligent Automation</span>
                </h1>
                <p class="hero-description">
                    Advanced computer vision system for PCB component pick-and-place operations
                    with 98.8% success rate, &plusmn;0.05mm placement tolerance, and real-time
                    6-DOF grasp planning for SMD components.
                </p>
            </div>
        </div>
    </section>

    <!-- Case Study Content -->
    <section class="case-study-content">
        <div class="container">
            <!-- Project Context -->
            <div class="case-study-section">
                <h2>Project Context</h2>
                <div class="tech-stack">
                    <div class="tech-item">
                        <h4>Client</h4>
                        <p>A consumer electronics contract manufacturer in Shenzhen with UK engineering office</p>
                    </div>
                    <div class="tech-item">
                        <h4>Timeline</h4>
                        <p>7-month engagement, April &ndash; October 2023</p>
                    </div>
                    <div class="tech-item">
                        <h4>Team</h4>
                        <p>1 robotics vision specialist, 1 controls engineer</p>
                    </div>
                </div>
                <p>The client manufactures consumer electronics for several major brands and needed to increase
                    throughput on their SMD (surface-mount device) component placement lines. Their existing
                    pick-and-place machines handled standard components well, but a growing proportion of their
                    product mix involved irregularly-shaped connectors, shielding cans, and non-standard packages
                    that defeated the fixed-template vision systems on their legacy machines. The UK engineering
                    office managed the vision system R&amp;D, with deployment and integration handled at the
                    Shenzhen production facility.</p>
            </div>

            <!-- Project Overview -->
            <div class="case-study-section">
                <h2>Project Overview</h2>
                <p>Developed a sophisticated robotic vision system for automated PCB component pick-and-place
                    operations. The system picks SMD components from feeder trays and places them on PCBs with
                    &plusmn;0.05mm tolerance, combining advanced computer vision, machine learning, and 6-DOF
                    robotic control to handle 50+ component types including irregularly-shaped connectors and
                    shielding cans that defeated the client's legacy template-matching systems.</p>

                <div class="tech-stack">
                    <div class="tech-item">
                        <h4>Hardware</h4>
                        <p>Universal Robots UR5e, Intel RealSense, Industrial Cameras</p>
                    </div>
                    <div class="tech-item">
                        <h4>AI Framework</h4>
                        <p>ROS2, OpenCV, PyTorch, MoveIt</p>
                    </div>
                    <div class="tech-item">
                        <h4>Computer Vision</h4>
                        <p>YOLOv7, Point Cloud Processing, 3D Reconstruction</p>
                    </div>
                    <div class="tech-item">
                        <h4>Control Systems</h4>
                        <p>PID Control, Trajectory Planning, Force Feedback</p>
                    </div>
                </div>
            </div>

            <!-- Challenge -->
            <div class="case-study-section">
                <h2>The Challenge</h2>
                <p>The client needed to automate the placement of non-standard SMD components &mdash; irregularly-shaped
                    connectors, metal shielding cans, and custom packages &mdash; that their existing template-based
                    pick-and-place machines could not handle. Key challenges included:</p>
                <ul>
                    <li>Detecting and classifying 50+ different SMD component types from feeder trays</li>
                    <li>Handling components with varying shapes, sizes, and orientations (0.4mm pitch QFPs to 20mm shielding cans)</li>
                    <li>Achieving &plusmn;0.05mm placement precision on PCB pads</li>
                    <li>Maintaining detection accuracy under production-floor lighting conditions</li>
                    <li>Integrating with existing conveyor and feeder tray systems</li>
                    <li>Ensuring 24/7 operation with minimal maintenance across multiple production lines</li>
                </ul>

                <h3>What Had Been Tried Before</h3>
                <p>The client's legacy pick-and-place machines used fixed template matching for component detection.
                    This worked reliably for standard rectangular passives and ICs but failed on the growing range of
                    non-standard components in their product mix &mdash; shielding cans with variable reflectivity,
                    connectors with protruding pins, and custom mechanical parts. The template library required manual
                    re-tuning for each new component, taking 2&ndash;3 days per component type, and even after tuning
                    the detection rate for irregular shapes rarely exceeded 88%. The client had also trialled a
                    commercial AI-based vision add-on, but it was optimised for warehouse logistics (large objects,
                    loose tolerance) and could not achieve the sub-millimetre precision required for PCB assembly.</p>
            </div>

            <!-- Solution -->
            <div class="case-study-section">
                <h2>Our Solution</h2>
                <h3>1. Multi-Camera Vision System</h3>
                <p>Deployed a synchronised multi-camera setup with RGB and structured-light depth cameras to capture
                    comprehensive 3D information about components in feeder trays and on PCBs. The system uses
                    structured light rather than stereo vision for depth estimation.</p>
                <p><strong>Why structured light over stereo:</strong> At the sub-millimetre scale of SMD components,
                    stereo matching struggles with the low-texture surfaces of bare PCBs and metallic component bodies.
                    Structured-light projection provides reliable depth at 0.02mm resolution regardless of surface
                    texture, which was essential for accurate pose estimation of reflective shielding cans.</p>

                <h3>2. Advanced Object Detection & Classification</h3>
                <p>Developed a custom YOLOv7 model trained on a large-scale dataset of electronic components. 100,000+
                    training images were collected over 4 weeks using an automated capture rig that photographed
                    components in feeder trays under varied orientations and lighting. Synthetic augmentation generated
                    an additional 300,000 samples covering edge-case orientations and partial occlusions. Manual
                    annotation of edge cases &mdash; particularly components with ambiguous orientation markers &mdash;
                    required 2 weeks of specialist labelling by engineers familiar with PCB assembly. The model
                    achieves 98.8% detection and classification accuracy across all 50+ component types.</p>
                <p><strong>Why YOLOv7 over alternatives:</strong> We benchmarked YOLOv7 against EfficientDet and
                    Detectron2 (Faster R-CNN). EfficientDet achieved comparable accuracy but at 3x the inference
                    latency, which would have pushed cycle time above the 2.5-second target. Faster R-CNN was more
                    accurate on the smallest components (0201 passives) but could not run at the required frame rate
                    on the target GPU (NVIDIA RTX A4000). YOLOv7 provided the best accuracy-latency trade-off for
                    our component size range.</p>

                <h3>3. 3D Pose Estimation</h3>
                <p>Implemented 6-DOF pose estimation combining point cloud registration with learned keypoint
                    detection, enabling the robot to determine each component's exact position and orientation
                    with sub-millimetre accuracy for precise placement on PCB pads.</p>
                <p><strong>Why hybrid over pure learned pose:</strong> Pure deep-learning pose estimators (e.g.,
                    PoseCNN) had difficulty generalising across the wide range of component geometries without
                    per-component fine-tuning. Our hybrid approach uses learned keypoints to initialise an ICP
                    (Iterative Closest Point) refinement step against CAD models, achieving consistent &plusmn;0.05mm
                    accuracy across all component types without component-specific training.</p>

                <h3>4. Intelligent Grasp Planning</h3>
                <p>Created a grasp planning system that analyses component geometry from the 3D point cloud and
                    selects optimal grasp points based on stability, accessibility, and collision avoidance with
                    adjacent components in feeder trays.</p>
                <p><strong>Why analytical over learned grasping:</strong> Learned grasp planners (e.g., GraspNet)
                    are effective for novel objects but introduce variability that is unacceptable at &plusmn;0.05mm
                    placement tolerance. Our analytical planner uses component CAD models to compute deterministic
                    grasp poses, guaranteeing repeatable pick orientation. For components without CAD models, we
                    fall back to a constrained learned planner that is limited to a pre-validated set of grasp
                    templates.</p>
            </div>

            <!-- Technical Implementation -->
            <div class="case-study-section">
                <h2>Technical Implementation</h2>
                <h3>Vision Pipeline</h3>
                <p>The vision system processes data through multiple stages:</p>
                <ul>
                    <li><strong>Image Acquisition:</strong> Synchronized capture from multiple cameras</li>
                    <li><strong>Preprocessing:</strong> Calibration, distortion correction, and enhancement</li>
                    <li><strong>Object Detection:</strong> YOLO-based detection and classification</li>
                    <li><strong>3D Reconstruction:</strong> Point cloud generation and processing</li>
                    <li><strong>Pose Estimation:</strong> 6DOF pose calculation for each object</li>
                </ul>

                <h3>Robotic Control</h3>
                <p>Advanced control algorithms ensure precise manipulation:</p>
                <ul>
                    <li>Trajectory planning with collision avoidance</li>
                    <li>Force feedback for delicate object handling</li>
                    <li>Adaptive control for varying object properties</li>
                    <li>Error recovery and retry mechanisms</li>
                </ul>

                <h3>Limitations & Edge Cases</h3>
                <ul>
                    <li><strong>Transparent & Reflective Components:</strong> Transparent components (glass display
                        covers) and highly reflective surfaces (polished metal shields) reduce detection accuracy
                        to 94.2%. The structured-light depth sensor produces noisy point clouds on these surfaces,
                        degrading pose estimation. We mitigate this with polarised lighting and surface-specific
                        detection thresholds, but it remains the primary failure mode.</li>
                    <li><strong>Controlled Lighting Requirement:</strong> The system requires controlled lighting
                        conditions. Ambient light variations above 200 lux (e.g., from skylights or open loading
                        bay doors) cause measurable drift in pose estimation, increasing placement error by up
                        to 0.03mm. Production cells must be enclosed or fitted with consistent LED overhead
                        lighting.</li>
                    <li><strong>New Component Onboarding:</strong> Adding a new component type requires a CAD model
                        (or 200+ annotated training images if no CAD is available) and approximately 4 hours of
                        calibration and validation. This is significantly faster than the 2&ndash;3 days required
                        by the legacy template system but is not instantaneous.</li>
                    <li><strong>Vacuum Gripper Limitations:</strong> Components smaller than 0.4mm x 0.2mm (0201
                        package size) cannot be reliably picked with the current vacuum nozzle. Handling these
                        requires a dedicated micro-placement head, which is outside the current system scope.</li>
                    <li><strong>Thermal Drift:</strong> After 8+ hours of continuous operation, thermal expansion
                        of the camera mounting bracket introduces up to 0.02mm systematic offset. The system
                        runs an automated recalibration cycle every 6 hours to compensate.</li>
                </ul>
            </div>

            <!-- Results -->
            <div class="case-study-section">
                <h2>Results & Impact</h2>
                <div class="tech-stack">
                    <div class="tech-item">
                        <h4>98.8%</h4>
                        <p>Success Rate (up from 88% with legacy template matching)</p>
                    </div>
                    <div class="tech-item">
                        <h4>&plusmn;0.05mm</h4>
                        <p>Placement Accuracy (was &plusmn;0.5mm with legacy system)</p>
                    </div>
                    <div class="tech-item">
                        <h4>2.3s</h4>
                        <p>Full Pick-Place-Verify Cycle Time</p>
                    </div>
                    <div class="tech-item">
                        <h4>50+</h4>
                        <p>Component Types (was 12 with legacy system)</p>
                    </div>
                </div>

                <h3>Performance Metrics (Before / After)</h3>
                <ul>
                    <li>Detection accuracy on non-standard components: <strong>88% &rarr; 98.8%</strong></li>
                    <li>Placement accuracy: <strong>&plusmn;0.5mm &rarr; &plusmn;0.05mm</strong> (10x improvement)</li>
                    <li>Supported component types: <strong>12 &rarr; 50+</strong> without per-component manual tuning</li>
                    <li>New component onboarding time: <strong>2&ndash;3 days &rarr; 4 hours</strong></li>
                    <li>System uptime: <strong>94% &rarr; 99.4%</strong> with automated error recovery and retry</li>
                </ul>

                <h3>Cycle Time Breakdown</h3>
                <p>The 2.3-second cycle time reflects the full pick-place-verify loop for precision PCB assembly,
                    not just the pick or place motion. The breakdown is as follows:</p>
                <ul>
                    <li><strong>Grasp planning:</strong> 0.8s &mdash; evaluating multiple grasp candidates for
                        irregularly-shaped components, selecting the most stable vacuum nozzle contact point</li>
                    <li><strong>Pick motion + vacuum engagement:</strong> 0.4s</li>
                    <li><strong>Transfer + 6-DOF alignment:</strong> 0.5s &mdash; including in-transit rotation
                        to match target pad orientation</li>
                    <li><strong>Precision placement at &plusmn;0.05mm tolerance:</strong> 0.3s &mdash; final
                        approach with force feedback to confirm contact</li>
                    <li><strong>Post-placement visual verification:</strong> 0.3s &mdash; confirming component
                        is correctly seated before proceeding to next pick</li>
                </ul>
                <p>For standard rectangular passives with known orientation, grasp planning is deterministic
                    and the cycle time drops to 1.6 seconds.</p>
            </div>

            <!-- Capabilities -->
            <div class="case-study-section">
                <h2>System Capabilities</h2>
                <p>The robotic vision system can handle various tasks:</p>
                <ul>
                    <li><strong>Object Detection:</strong> Identify and locate components in cluttered environments</li>
                    <li><strong>Classification:</strong> Distinguish between different component types and variants</li>
                    <li><strong>Pose Estimation:</strong> Determine 6DOF pose for precise manipulation</li>
                    <li><strong>Grasp Planning:</strong> Select optimal grasp points for stable manipulation</li>
                    <li><strong>Quality Inspection:</strong> Detect defects and quality issues during handling</li>
                    <li><strong>Adaptive Behavior:</strong> Learn and adapt to new component types</li>
                </ul>
            </div>

            <!-- Integration -->
            <div class="case-study-section">
                <h2>System Integration</h2>
                <p>Seamlessly integrated with existing manufacturing infrastructure:</p>
                <ul>
                    <li><strong>Production Line Integration:</strong> Direct communication with conveyor systems</li>
                    <li><strong>Quality Control:</strong> Integration with inspection and testing systems</li>
                    <li><strong>Data Management:</strong> Real-time data logging and analytics</li>
                    <li><strong>Maintenance Systems:</strong> Predictive maintenance and health monitoring</li>
                    <li><strong>Safety Systems:</strong> Integration with safety sensors and emergency stops</li>
                </ul>
            </div>

            <!-- Applications -->
            <div class="case-study-section">
                <h2>Deployment Scope</h2>
                <p>The system is deployed across the client's PCB assembly operations:</p>
                <ul>
                    <li><strong>Non-Standard SMD Placement:</strong> Primary application &mdash; irregularly-shaped
                        connectors, shielding cans, and custom mechanical components that defeat template matching</li>
                    <li><strong>Mixed-Component Trays:</strong> Handling feeder trays with multiple component types
                        in a single pass, reducing tray changeover downtime</li>
                    <li><strong>Post-Reflow Inspection:</strong> Visual verification of component placement after
                        solder reflow, flagging misaligned or tombstoned components</li>
                    <li><strong>Rework Assistance:</strong> Identifying and picking misplaced components for rework,
                        guided by inspection system feedback</li>
                </ul>
            </div>

            <!-- Ongoing & Next Steps -->
            <div class="case-study-section">
                <h2>Ongoing & Next Steps</h2>
                <p>Active development and planned near-term improvements:</p>
                <ul>
                    <li><strong>In progress:</strong> Expanding the training dataset with polarised-light captures
                        to improve detection of transparent and reflective components (targeting 97%+ accuracy,
                        up from current 94.2%)</li>
                    <li><strong>In progress:</strong> Online learning pipeline that incorporates production-line
                        failure cases into nightly model retraining, reducing manual annotation effort for new
                        edge cases</li>
                    <li><strong>Planned (Q1 2024):</strong> Dual-arm coordination for simultaneous pick-and-place
                        on opposite sides of the PCB, targeting 40% cycle time reduction for double-sided boards</li>
                    <li><strong>Under evaluation:</strong> Integration with solder paste inspection (SPI) data
                        to adapt placement force based on paste volume, reducing defect rates on fine-pitch
                        components</li>
                    <li><strong>Under evaluation:</strong> Deploying a second system at the client's Dongguan
                        facility, which would require adapting the lighting enclosure for a different production
                        line layout</li>
                </ul>
            </div>
        </div>
    </section>

</body>

</html>